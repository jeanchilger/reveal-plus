#!/bin/bash

export ABS_PATH=`pwd`

source "${ABS_PATH}/handle_errors"
source "${ABS_PATH}/colors"


PURPOSE=baseline
JUDGECLASS=("toyDataset");


CORPLIST=("${JUDGECLASS}")
SOFIA="/home/jean/Documents/tar-toolkit-core-copy/sofia-ml-read-only/sofia-ml"

: '
    Iterate over items in CORPLIST
'
for CORP in "${CORPLIST[@]}"; do

    pushd Corpus # pushd <dir> é semelhante à cd <dir>

    echo -e "${BLUE}Preparando dataset...${END}"

    ./dofast4 "$CORP"

    cp "$CORP".df ../"$CORP".df

    cp "$CORP".svm.fil ../"$CORP".svm.fil

    popd # popd é semelhante ao cd ..
    : '
        Iterates over all `topic:query` pairs within
        judgement/$CORP.topic.stemming.txt
    '
    while IFS='' read -r line || [[ -n $line ]]; do

        IFS=':' read -ra TEXT <<< "$line"

        TOPIC="${TEXT[0]}"
        QUERY="${TEXT[1]}"

        : '
            Checks if the TOPIC or QUERY are empty
        '
        if [[ -z $TOPIC ]]; then
            throw $EMPTY_VARIABLE_EXCEPTION "Variable TOPIC is empty"

        elif [[ -z $QUERY ]]; then
            throw $EMPTY_VARIABLE_EXCEPTION "Variable QUERY is empty"

        fi

        echo -e "${WHITE}Topic${END}:$TOPIC"
        echo -e "${WHITE}Query${END}:$QUERY"

        ######
        try "Creating directories to store results..."
        (
            rm -rf result/"$PURPOSE"/"$CORP"/"$TOPIC"/
            mkdir -p result/"$PURPOSE"/"$CORP"/
            mkdir -p result/dump/"$PURPOSE"/"$CORP"/

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        echo -e "${GREEN}Done.${END}"

        ######
        try "Creating topic directory topic (${TOPIC})..."
        (
            rm -rf $TOPIC
            mkdir $TOPIC

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        echo -e "${GREEN}Done.${END}"

        ######
        try "Creating file 'N' with number of documents..."
        (
            : '
                Creates a `N` file with number of all docs
            '
            echo `wc -l < "$CORP".svm.fil` > N

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        echo -e "${GREEN}Done.${END}"

        pushd $TOPIC

        echo "$QUERY" > "$TOPIC".seed.doc

        ######
        try "Preparing 'docfils' file with all documents..."
        (
            cut -d' ' -f1 ../$CORP.svm.fil | sed -e 's/.*/& &/' > docfil
            cut -d' ' -f1 docfil | cat -n > docfils

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        echo -e "${GREEN}Done.${END}"

        : '
            The `new$N` files keeps the files used on
                the $N-th iteration
        '

        ######
        try "Preparing relevance calculation files..."
        (
            touch rel.$TOPIC.fil
            touch prel.$TOPIC

            rm -rf prevalence.rate
            touch prevalence.rate

            rm -rf rel.rate
            touch rel.rate

            rm -f new[0-9][0-9].$TOPIC tail[0-9][0-9].$TOPIC self*.$TOPIC gold*.$TOPIC
            touch new00.$TOPIC

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        echo -e "${GREEN}Done.${END}"

        # Total number of documents
        NDOCS=`cat docfils | wc -l`
        # Number of documents already labeled
        NDUN=0
        # Number of documents to be labeled in the current iteration
        L=1

        R=100
        export LAMBDA=0.0001

        : '
            $TOPIC.seed.doc stores the $TOPIC query
        '

        cp $TOPIC.seed.doc ../$TOPIC.seed.doc

        popd

        echo -e "${BLUE}Executing ./dofeaturesseed4...${END}"

        ./dofeaturesseed4 $TOPIC.seed.doc $TOPIC $CORP

        echo -e "${GREEN}Finished ./dofeaturesseed4${END}"

        pushd $TOPIC

        ######
        try "Preparing ../$CORP.svm.fil (runs ../dosplit) and $TOPIC.synthetic.seed..."
        (
            sed -e 's/[^ ]*/0/' ../$CORP.svm.fil | ../dosplit
            sed -e 's/[^ ]*/1/' svm.$TOPIC.seed.doc.fil > $TOPIC.synthetic.seed

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        echo -e "${GREEN}Done.${END}"

        for x in 0 1 2 3 4 5 6 7 8 9 ; do

            for y in 0 1 2 3 4 5 6 7 8 9 ; do

                if [ $NDUN -lt $NDOCS ] ; then
                    export N=$x$y

                    ######
                    try "Preparing trainset..."
                    (
                        cp $TOPIC.synthetic.seed trainset

                        cut -f2 docfils | sort -R | head -$R | sort | join - ../$CORP.svm.fil | sed -e's/[^ ]*/-1/' >> trainset

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    echo -e "${GREEN}Done.${END}"

                    ######
                    try "Preparing seed (and x)..."
                    (
                        cat sub_new[0-9][0-9].$TOPIC > seed
                        cat seed | sort | join - rel.$TOPIC.fil | sed -e 's/^/1 /' > x
                        cat seed | sort | join -v1 - rel.$TOPIC.fil | sort -R | head -50000 | sed -e 's/^/-1 /' >> x

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    echo -e "${GREEN}Done.${END}"

                    ######
                    try "Preparing trainset (pt 2)..."
                    (
                        sort -k2 x | join -12 - ../$CORP.svm.fil | cut -d' ' -f2- | sort -n >> trainset

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    echo -e "${GREEN}Done.${END}"

                    # Calculate relevant documents prevalence rate in the traning set
                    RELTRAINDOC=`grep -E "^1\b" trainset | wc -l`

                    NOTRELTRAINDOC=`grep -E "^-1\b" trainset | wc -l`

                    PREVALENCERATE=`echo "scale=4; $RELTRAINDOC / ($RELTRAINDOC + $NOTRELTRAINDOC)" | bc`

                    echo $RELTRAINDOC $NOTRELTRAINDOC $PREVALENCERATE >> prevalence.rate

                    ######
                    try "Training (Running SOFIA-ML)..."
                    (

                        $SOFIA --learner_type logreg-pegasos --loop_type roc --lambda $LAMBDA --iterations 200000 --training_file trainset --dimensionality 9300000 --model_out svm_model

                        RES=$?

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    echo -e "${GREEN}Finished SOFIA-ML.${END}"

                    echo $RES

                    if [[ "$RES" -eq "0" ]] ; then
                        for z in svm.test.* ; do

                            ######
                            try "Testing previously created model (running sofia-ml) $z..."
                            (
                                $SOFIA --test_file $z --dimensionality 9300000 --model_in svm_model --results_file pout.$z

                            ) 2> $STD_ERROR_OUT

                            catch || {
                                echo $ERROR_CODE
                                exit_on_error
                            }

                            echo -e "${GREEN}Finished testing with $z.${END}"
                        done

                    else
                        ######
                        try "Preparing pout.svm.test.1..."
                        (
                            rm -f pout.svm.test.*
                            cut -f2 docfils | sort -R | cat -n | sort -k2 | sed -e 's/ */-/' > pout.svm.test.1

                        ) 2> $STD_ERROR_OUT

                        catch || {
                            exit_on_error
                        }

                        echo -e "${GREEN}Done.${END}"
                    fi

                    ######
                    try "Starting SCAL process $N\nPreparing ranking.$N.$TOPIC..."
                    (
                        cat new[0-9][0-9].$TOPIC > seed.$TOPIC
                        cut -f1 pout.svm.test.* | ../fixnum | cat -n | join -o2.2,1.2 -t$'\t' - docfils | sort -k1 -n  > inlr.out.$N.$TOPIC

                        echo -e "${BLUE}\tinlr.out size =`wc -l < inlr.out.$N.$TOPIC` docfils  `wc -l <docfils `${END}"

                        sort -n seed.$TOPIC > temp
                        cat temp | join  -v2 - inlr.out.$N.$TOPIC -2 1 | shuf |  sort -k 2 -r -g -s  > ranking.$N.$TOPIC


                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    echo -e "${GREEN}Done.${END}"

                    ######
                    try "Preparing new$N.$TOPIC..."
                    (
                        echo -e "${BLUE}\tranking size `wc -l < ranking.$N.$TOPIC`${END}"
                        cat ranking.$N.$TOPIC | cut -d' ' -f1 > new$N.$TOPIC
                        cp new$N.$TOPIC U$N

                        cat new[0-9][0-9].$TOPIC > x

                        if [ "$N" != "99" ] ; then
                            head -$L new$N.$TOPIC > y
                            mv y new$N.$TOPIC
                        fi

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    echo -e "${GREEN}Done.${END}"

                    : '
                        x armazena new($N - 1).$TOPIC
                    '

                    # Limits the number of documents by 30
                    if [ $L -le 30 ]; then
                        b=$L
                    else
                        b=30
                    fi

                    : '
                        The files sub_new$N represents a sample of at most 30
                            documents from the documents to be used in the iteration.
                    '
                    shuf -n $b new$N.$TOPIC > sub_new$N.$TOPIC

                    echo -e "${BLUE}Number of labeled pairs: `wc -l < sub_new$N.$TOPIC`${END}"

                    # judgefile tells which are the positive docs
                    python2.7 ../doJudgementMain.py --topic=$TOPIC --judgefile=../judgement/qrels.$JUDGECLASS.list --input=sub_new$N.$TOPIC --output=rel.$TOPIC.Judged.doc.list --record=$TOPIC.record.list

                    # rel.$TOPIC.Judged.doc.list contains the current relevant docs

                    cat rel.$TOPIC.Judged.doc.list >> rel.$TOPIC.fil
                    cat rel.$TOPIC.Judged.doc.list > rel.$TOPIC.$N.Judged.doc.list

                    RELFINDDOC=`wc -l < rel.$TOPIC.Judged.doc.list`

                    alreadyLabeledDocs=`cat sub_new[0-9][0-9].$TOPIC  | wc -l`
                    allDocs=`cat new[0-9][0-9].$TOPIC  | wc -l`

                    echo "rel $RELFINDDOC L $L b $b  alreadyLabeledDocs $alreadyLabeledDocs  allDocs $allDocs REL $RELRATE CURRENTREL $CURRENTREL relevant docs `wc -l < rel.$TOPIC.fil` "

                    aux=$((($RELFINDDOC*$L)/$b))
                    Rel=$(($Rel+$aux*1000))

                    RELRATE=`echo "scale=4; $RELFINDDOC / $L" | bc`
                    CURRENTREL=`wc -l < rel.$TOPIC.fil`
                    echo $RELFINDDOC $L $b $RELRATE $CURRENTREL >> rel.rate

                    sort rel.$TOPIC.fil | sed -e 's/$/ 1/' > prel.$TOPIC

                    cut -d' ' -f1 prel.$TOPIC > rel.$TOPIC.fil

                    NDUN=$((NDUN+L))
                    L=$((L+(L+9)/10))
                fi
            done
        done

        rm -rf svm.test.*
        popd

        mv $TOPIC result/"$PURPOSE"/"$CORP"/$TOPIC
        rm $TOPIC.seed.doc

    done < "judgement/$CORP.topic.stemming.txt"

    echo -e "${WHITE}Apos finalizar o método o numero de docs recuperados do método  será o top total documentos acessados até a ultima executação que encontrou documentos relevantes. Por exemplo, se a execução N =20 foi a ultima a achar documentos releventas na amostragem deve-se pegar os top ~300 como relevantes. Essa será saida do método. Ou seja, o arquivo rel.rate armazena os relevantes recuperados, é só usar ele para implementar isso.${END}"


    rm -rf "$CORP".svm.fil
    rm "$CORP".df

    rm N

    # Generate LSI from tfdf
    # python clustering/doLSI.py --input=tfdf_oldreut --output=LSIVector/"$CORP".lsi.dump --mapping=LSIVector/"$CORP".mapping.dump --latent=200 --choice=entropy --normalization=yes

done
